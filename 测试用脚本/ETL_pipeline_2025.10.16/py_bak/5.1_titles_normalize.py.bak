# -*- coding: utf-8 -*-
# åˆ›å»ºæ—¶é—´: 2025/10/24
"""
=============================================================
æ ‡é¢˜ç­‰çº§æ¢³ç†è„šæœ¬ï¼ˆé€è¡Œè¯†åˆ«ç‰ˆ + ä¿®æ­£ç‰ˆåŠ ç²—é€»è¾‘ï¼‰
Version: ETL_pipeline_2025.10.24/5_titles_normalize_v11.py
=============================================================

åŠŸèƒ½æ¦‚è¿°ï¼š
-------------------------------------------------------------
1. æŒ‰è¡Œè¯†åˆ« H1~H4ï¼›
2. è‹¥è¡Œé¦–å¸¦å•ä¸ª # ä½†ä¸åœ¨ H1 è§„åˆ™ä¸­ï¼ˆå‰è¨€ã€åè®°ã€ç¥è¯ã€ä¼ è¯´ã€æ•…äº‹ï¼‰ï¼Œ
   ä¸”æœªè¢«è¯†åˆ«ä¸º H1~H4 æ ‡é¢˜ â†’ å»æ‰ # å¹¶åŠ ç²— **...**ï¼›
3. è¾“å‡ºè§„èŒƒåŒ– Markdownã€ç»Ÿè®¡è¡¨ä¸åŠ ç²—æ˜ç»†è¡¨ï¼ˆå§‹ç»ˆç”Ÿæˆä¸¤ä»½ CSVï¼‰ã€‚
"""

import os
import re
import csv

# ==========================================================
# è·¯å¾„é…ç½®
# ==========================================================
INPUT_PATH    = r"I:\ä¸­å›½æ°‘é—´ä¼ ç»Ÿæ•…äº‹\åˆ†å·æ¸…æ´—\yunnan\Chinese Folk Tales_yunnan.md"
OUTPUT_TARGET = r"I:\ä¸­å›½æ°‘é—´ä¼ ç»Ÿæ•…äº‹\åˆ†å·æ¸…æ´—\yunnan\5_Chinese Folk Tales_yunnan.md"
CSV_PATH      = r"I:\ä¸­å›½æ°‘é—´ä¼ ç»Ÿæ•…äº‹\åˆ†å·æ¸…æ´—\yunnan\5_yunnan_heading_stats.csv"
CSV_EMPH_PATH = r"I:\ä¸­å›½æ°‘é—´ä¼ ç»Ÿæ•…äº‹\åˆ†å·æ¸…æ´—\yunnan\5_yunnan_emphasized_h1.csv"

DEBUG_SHOW_PER_LEVEL = 3

# ==========================================================
# å…¬å…±ç¬¦å·
# ==========================================================
PUNCT_WS = r"\s,\.ï¼Œã€‚:ï¼š;ï¼›!ï¼\?ï¼ŸÂ·ãƒ»â€”\-_\~`'\"â€œâ€â€˜â€™\(\)ï¼ˆï¼‰\[\]ã€ã€‘<>ã€Šã€‹ã€â€¦â‹¯ï¼Â·"
ROM_NUMS = "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶ã€‡â—‹"
RE_ALL_HASHES = re.compile(r"^\s{0,3}#{1,6}\s*")

# ==========================================================
# æ¨¡ç³ŠåŒ¹é…æ„å»ºå‡½æ•°
# ==========================================================
def build_fuzzy_regex(text: str) -> re.Pattern:
    core = re.sub(f"[{PUNCT_WS}]", "", text)
    if not core:
        return re.compile(r"^(?!)$")
    sep = f"[{PUNCT_WS}]*"
    pattern = rf"^\s*[{PUNCT_WS}]*" + sep.join(map(re.escape, core)) + rf"[{PUNCT_WS}]*\s*$"
    return re.compile(pattern)

# ==========================================================
# æ ‡é¢˜åŒ¹é…è§„åˆ™
# ==========================================================
H1_TITLES = ["å‰è¨€", "åè®°", "ç¥è¯", "ä¼ è¯´", "æ•…äº‹"]
H2_TITLES = [
    "å¼€å¤©è¾Ÿåœ°ç¥è¯","è‡ªç„¶å¤©è±¡ç¥è¯","åŠ¨ç‰©æ¤ç‰©ç¥è¯","åŠ¨æ¤ç‰©ç¥è¯","å›¾è…¾ç¥–å…ˆç¥è¯","ç¥–å…ˆç¥è¯","å¤©ä½“è‡ªç„¶ç¥è¯",
    "æ´ªæ°´äººç±»å†ç¹è¡ç¥è¯","æ–‡åŒ–èµ·æºç¥è¯","ç¥å’Œè‹±é›„ç¥è¯","è‹±é›„ç¥è¯",
    "äººç‰©ä¼ è¯´","ä¸‰å›½èœ€æ±‰äººç‰©ä¼ è¯´","æ–‡äººä¼ è¯´","ç°ä»£é©å‘½å®¶ä¼ è¯´",
    "å²äº‹ä¼ è¯´","åœ°æ–¹ä¼ è¯´","åå±±ä¼ è¯´","é£ä¿—ä¼ è¯´","åŠ¨æ¤ç‰©ä¼ è¯´","åŠ¨ç‰©æ¤ç‰©ä¼ è¯´","ä¸°éƒ½é¬¼åŸä¼ è¯´",
    "åœŸç‰¹äº§ä¼ è¯´","æ°‘é—´å·¥è‰ºä¼ è¯´",
    "åŠ¨ç‰©æ•…äº‹","å¹»æƒ³æ•…äº‹","é¬¼ç‹ç²¾æ€ªæ•…äº‹","ç”Ÿæ´»æ•…äº‹",
    "æœºæ™ºäººç‰©æ•…äº‹","å¯“è¨€æ•…äº‹","ç¬‘è¯","å¯“è¨€"
]

H1_PATTERNS = [build_fuzzy_regex(t) for t in H1_TITLES]
H2_PATTERNS = [build_fuzzy_regex(t) for t in H2_TITLES]
RE_H3_NUM_TITLE = re.compile(rf"^\s*(?P<num>\d{{1,5}})\s*[{PUNCT_WS}]*\s*(?P<title>.*[\u4e00-\u9fa5].*?)\s*$")
RE_H4_APPENDIX = re.compile(rf"^\s*[{PUNCT_WS}]*(é™„è®°|é™„[:ï¼š])")

# ==========================================================
# æ¸…ç†å‡½æ•°
# ==========================================================
def clean_lines(lines):
    return [line.rstrip("\n") + "\n" for line in lines]

# ==========================================================
# æ ‡é¢˜è¯†åˆ«å‡½æ•°
# ==========================================================
def detect_headings(lines):
    results = []
    for line in lines:
        text = line.rstrip("\n")
        stripped = text.strip()
        if not stripped:
            results.append({"level": None, "title": None, "text": line})
            continue

        core = RE_ALL_HASHES.sub("", stripped)
        lvl, title = None, None

        # H1
        for p in H1_PATTERNS:
            if p.match(core):
                lvl, title = 1, core
                break

        # H2
        if not lvl:
            for p in H2_PATTERNS:
                if p.match(core):
                    lvl, title = 2, core
                    break

        # H3
        if not lvl:
            m = RE_H3_NUM_TITLE.match(core)
            if m:
                num = int(m.group("num"))
                title_part = m.group("title")
                if num <= 750 and len(title_part) <= 15:
                    lvl, title = 3, core

        # H4 é™„ç±»
        if not lvl and RE_H4_APPENDIX.match(core):
            lvl, title = 4, core

        results.append({"level": lvl, "title": title, "text": line})
    return results

# ==========================================================
# ä¿®æ­£ç‰ˆï¼šå¸¦å•ä¸ª#ä¸”ä¸åŒ¹é…H1è§„åˆ™ â†’ åŠ ç²—
# ==========================================================
def emphasize_unmatched_h1(results):
    emphasized = []
    for item in results:
        line = item["text"].rstrip("\n")

        # åŸå§‹è¡Œä¸ºå•ä¸ª#
        if re.match(r"^\s*#(?!#)", line):
            core = re.sub(r"^\s*#\s*", "", line).strip()
            matched_h1 = any(p.match(core) for p in H1_PATTERNS)
            # è‹¥è¡Œæœªè¢«è¯†åˆ«ä¸ºä»»ä½•æ ‡é¢˜æˆ–ä¸æ˜¯åˆæ³•H1
            if (not item["level"]) or (item["level"] == 1 and not matched_h1):
                item["text"] = f"**{core}**\n"
                item["level"] = None
                emphasized.append(core)

    return results, emphasized

# ==========================================================
# è¾“å‡ºä¸ç»Ÿè®¡
# ==========================================================
def export_results(results, emphasized):
    counts = {1:0, 2:0, 3:0, 4:0}
    samples = {1:[], 2:[], 3:[], 4:[]}
    out_lines = []

    for item in results:
        lvl, title, line = item["level"], item["title"], item["text"].rstrip("\n")
        if lvl:
            out_lines.append("#" * lvl + " " + title + "\n")
            counts[lvl] += 1
            if len(samples[lvl]) < DEBUG_SHOW_PER_LEVEL:
                samples[lvl].append(title)
        else:
            out_lines.append(line + "\n")

    os.makedirs(os.path.dirname(OUTPUT_TARGET) or ".", exist_ok=True)
    with open(OUTPUT_TARGET, "w", encoding="utf-8") as f:
        f.writelines(out_lines)

    # è¾“å‡ºä¸¤ä¸ª CSVï¼ˆå§‹ç»ˆç”Ÿæˆï¼‰
    rows = [{
        "file": os.path.basename(INPUT_PATH),
        "H1": counts[1],
        "H2": counts[2],
        "H3": counts[3],
        "H4": counts[4],
        "Emphasized_H1": len(emphasized),
        "TOTAL": sum(counts.values())
    }]
    with open(CSV_PATH, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["file","H1","H2","H3","H4","Emphasized_H1","TOTAL"])
        writer.writeheader()
        writer.writerows(rows)

    # ç¬¬äºŒä¸ªCSVå³ä½¿ä¸ºç©ºä¹Ÿç”Ÿæˆ
    with open(CSV_EMPH_PATH, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Emphasized_H1_Text"])
        for t in emphasized:
            writer.writerow([t])

    summary = "ï¼Œ".join([f"H{lvl}={counts[lvl]}ï¼ˆä¾‹ï¼š{'ï¼›'.join(samples[lvl])}ï¼‰" for lvl in range(1,5) if counts[lvl]])
    print(f"âœ… æ ‡é¢˜è¯†åˆ«å®Œæˆ | {summary} | åŠ ç²—H1={len(emphasized)}")
    print(f"ğŸ“„ ç»Ÿè®¡æ–‡ä»¶: {CSV_PATH}")
    print(f"ğŸ“‘ åŠ ç²—H1æ˜ç»†: {CSV_EMPH_PATH}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_TARGET}")

# ==========================================================
# ä¸»æµç¨‹
# ==========================================================
def main():
    if not os.path.exists(INPUT_PATH):
        print(f"[ERROR] æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_PATH}")
        return

    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        lines = f.readlines()

    print("ğŸš€ é˜¶æ®µ1ï¼šé€è¡Œè¯†åˆ« H1~H4 â€¦")
    results = detect_headings(clean_lines(lines))

    print("âœ¨ é˜¶æ®µ2ï¼šå¤„ç†å¸¦#ä½†ä¸åœ¨H1è§„åˆ™çš„è¡Œ â†’ åŠ ç²— â€¦")
    results, emphasized = emphasize_unmatched_h1(results)

    print("ğŸ’¾ é˜¶æ®µ3ï¼šè¾“å‡º Markdown ä¸ CSV â€¦")
    export_results(results, emphasized)

if __name__ == "__main__":
    main()
